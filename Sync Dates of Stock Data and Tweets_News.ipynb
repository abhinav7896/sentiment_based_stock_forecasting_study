{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSFT.json\n",
      "TSLA.json\n",
      "GOOGL.json\n",
      "AAPL.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def timestamp(date):\n",
    "    return pd.Timestamp(date)\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "\n",
    "df = pd.read_csv('stocks.csv')\n",
    "tickers = {'MSFT': 'microsoft', 'TSLA': 'tesla', 'GOOGL':'google', 'AAPL':'apple'}\n",
    "stockdata = {}\n",
    "for ticker in tickers:\n",
    "    stockdata[ticker] = df.loc[df['Stock'] == ticker]\n",
    "\n",
    "for ticker in tickers:\n",
    "    stock = stockdata[ticker]\n",
    "    tn = pd.read_json('tweetsnnews.json').dropna().to_dict()\n",
    "    tn = pd.DataFrame.from_dict(tn[tickers[ticker]], orient='index').transpose()\n",
    "    stockdic = stock.to_dict()\n",
    "    tndic = tn.to_dict()\n",
    "    stockdic['Daily_Info'] = {}\n",
    "    for i in stockdic['Date']:\n",
    "        date = stockdic['Date'][i]\n",
    "        stockdic['Daily_Info'][i] = tndic.get(timestamp(date))\n",
    "    df = pd.DataFrame.from_dict(stockdic)\n",
    "    filename = ticker+'.json'\n",
    "    print(filename)\n",
    "    df.to_json(filename)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Abhinav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def clean(text, remove_stopwords = True):\n",
    "  CURRENCIES = {\n",
    "    \"$\": \"USD\", \"zł\": \"PLN\", \"£\": \"GBP\", \"¥\": \"JPY\", \"฿\": \"THB\", \"₡\": \"CRC\", \"₦\": \"NGN\",\"₩\": \"KRW\",\n",
    "    \"₪\": \"ILS\", \"₫\": \"VND\", \"€\": \"EUR\", \"₱\": \"PHP\", \"₲\": \"PYG\", \"₴\": \"UAH\", \"₹\": \"INR\",}\n",
    "  CURRENCY_REGEX = re.compile(\n",
    "    \"({})+\".format(\"|\".join(re.escape(c) for c in CURRENCIES.keys())))\n",
    "  \n",
    "  EMAIL_REGEX = re.compile(\n",
    "    r\"(?:^|(?<=[^\\w@.)]))([\\w+-](\\.(?!\\.))?)*?[\\w+-]@(?:\\w-?)*?\\w+(\\.([a-z]{2,})){1,3}(?:$|(?=\\b))\",\n",
    "    flags=re.IGNORECASE | re.UNICODE,)\n",
    "\n",
    "  # A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "  contractions =          {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\", \"i've\": \"i have\"}\n",
    "  text = text.lower()\n",
    "  if True:\n",
    "    text = text.split()\n",
    "    new_text = []\n",
    "    for word in text:\n",
    "      if word in contractions:\n",
    "        new_text.append(contractions[word])\n",
    "      else:\n",
    "          new_text.append(word)\n",
    "    text = \" \".join(new_text)\n",
    "        \n",
    "  text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "  text = EMAIL_REGEX.sub(' ',text)\n",
    "  text = CURRENCY_REGEX.sub(' ',text)\n",
    "  text = ' '.join([contractions[t] if t in contractions else t for t in text.split(\" \")])    \n",
    "  text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "  text = re.sub(r\"'s\\b\",\"\", text)\n",
    "  text = re.sub(r'&amp;', '', text) \n",
    "\n",
    "  if remove_stopwords:\n",
    "      text = text.split()\n",
    "      stops = set(stopwords.words(\"english\"))\n",
    "      text = [w for w in text if not w in stops]\n",
    "      text = \" \".join(text)\n",
    "  \n",
    "  return text\n",
    "\n",
    "def cleanText(text):\n",
    "  if(text != None):\n",
    "      text = clean(text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSFT_processed.json\n",
      "TSLA_processed.json\n",
      "GOOGL_processed.json\n",
      "AAPL_processed.json\n"
     ]
    }
   ],
   "source": [
    "# Preprocess Daily Info\n",
    "\n",
    "tickers = {'MSFT': 'microsoft', 'TSLA': 'tesla', 'GOOGL':'google', 'AAPL':'apple'}\n",
    "\n",
    "for ticker in tickers:\n",
    "    df = pd.read_json(ticker+'.json')\n",
    "    dic = df.to_dict()\n",
    "    df = pd.DataFrame.from_dict(dic) \n",
    "    dic['Processed_Daily_Info'] = dic['Daily_Info'].copy()\n",
    "    for item in dic['Daily_Info']:\n",
    "        info = dic['Daily_Info'].get(item)\n",
    "        if(info != None):\n",
    "            for i in info:\n",
    "                cleaned = cleanText(info[i])\n",
    "                dic['Processed_Daily_Info'][item][i] = cleaned\n",
    "    df = pd.DataFrame.from_dict(dic) \n",
    "    df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n",
    "    filename = ticker+'_processed.json'\n",
    "    print(filename)\n",
    "    df.to_json(filename) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
